# -*- coding: utf-8 -*-
"""LMG_gesture_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zhV4IpHuyrCTNZGEdfadjC8IZYznVWqV
"""

import torch
import torch.nn
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

pd.set_option('future.no_silent_downcasting', True)

input_size = 5
output_size = 4
hidden_size = 32
num_layers = 2
dropout = 0.2
bidirectional = True

learning_rate = 0.01
epochs = 5
freq = 100
batch_size = 4
n_splits = 3
window_size = 10
stride = 5
weight_decay = 0.0001

class_names = [0, 1, 2, 3]

data_folder = "/content/drive/MyDrive/LMG/datasets"

gest_map = {
      "rahat": 0,
      "yumruk": 1,
      "ön": 2,
      "arka": 3
    }

total_label_counts = {0: 0, 1: 0, 2: 0, 3: 0}
label_counts = {}

for file in os.listdir("/content/drive/MyDrive/LMG/datasets"):

  df = pd.read_csv("/content/drive/MyDrive/LMG/datasets/"+file)
  df.replace({"label": gest_map}, inplace=True)

  label_counts.update(df["label"].value_counts().sort_index())

  for key, count in label_counts.items():
        total_label_counts[key] += count

labels = total_label_counts.keys()
sizes = total_label_counts.values()

plt.figure(figsize=(7, 7))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
plt.title('Distribution of Labels')
plt.axis('equal')
plt.show()

class LMGDataset(Dataset):
  def __init__(self, data_folder, window_size, stride):

    self.data_folder = data_folder
    self.files = [file for file in os.listdir(self.data_folder)]
    self.window_size = window_size
    self.stride = stride
    self.scaler = MinMaxScaler()

    self.data_arr = []
    self.total_len = 0

    for file in self.files:
      df = pd.read_csv("/content/drive/MyDrive/LMG/datasets/"+file)

      data = df.iloc[:, :-1].values
      self.data_arr.append(data)

      self.total_len += max(0, (len(df) - self.window_size) // self.stride + 1)

    all_data = np.vstack(self.data_arr)
    self.scaler = self.scaler.fit(all_data)

  def __getitem__(self, idx):
    gest_map = {
      "rahat": 0,
      "yumruk": 1,
      "ön": 2,
      "arka": 3
    }

    cumulative_len = 0

    for file in self.files:
      df = pd.read_csv("/content/drive/MyDrive/LMG/datasets/"+file)
      df.replace({"label": gest_map}, inplace=True)

      data = df.iloc[:, :-1].values
      self.scaler.transform(data)

      num_windows = max(0, (len(df) - self.window_size) // self.stride + 1)
      #print(f"\nFile: {file} | Rows: {len(df)} | Windows: {num_windows}\n")

      if idx < cumulative_len + num_windows:
        local_idx = idx - cumulative_len
        start = local_idx * self.stride
        seq_x = data[start : start+window_size]
        seq_y = df.iloc[start + window_size - 1, -1]

        return torch.tensor(seq_x, dtype=torch.float32), torch.tensor(seq_y, dtype=torch.long)

      cumulative_len += num_windows

    if idx == self.__len__():
      raise IndexError("Index out of range.")

  def __len__(self):

    return self.total_len

class LSTMModel(nn.Module):
  def __init__(self, input_size, output_size,
               hidden_size, num_layers, dropout, bidirectional):
    super().__init__()

    self.lstm = nn.LSTM(
          input_size,
          hidden_size,
          num_layers,
          dropout=dropout,
          bidirectional=bidirectional,
          batch_first=True
        )

    self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, output_size)

  def forward(self, x):
    lstm_out, (h_n, c_n) = self.lstm(x)

    if self.lstm.bidirectional:
        lstm_out = lstm_out[:, -1, :].view(-1, self.lstm.hidden_size * 2)
    else:
        lstm_out = lstm_out[:, -1, :].view(-1, self.lstm.hidden_size)

    out = self.fc(lstm_out)

    return out

model = LSTMModel(
      input_size,
      output_size,
      hidden_size,
      num_layers,
      dropout,
      bidirectional
    )

optimizer = optim.SGD(model.parameters(), learning_rate, weight_decay=weight_decay)
scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)
loss_fn = nn.CrossEntropyLoss()

def plot_confusion_matrix(labels, preds, class_names):
    cm = confusion_matrix(labels, preds)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix")
    plt.show()

def train_batch(model, train_loader, optimizer,
                loss_fn, freq, device):

  total_loss = 0.
  global_step = 0
  correct_preds = 0
  total_preds = 0
  accuracy = 0
  last_loss = 0.

  pred_arr = []
  true_arr = []

  for x_batch, y_batch in train_loader:

      x_batch , y_batch = x_batch.to(device), y_batch.to(device)

      y_pred = model(x_batch)

      loss = loss_fn(y_pred, y_batch.type(torch.long).to(device))
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      lr = optimizer.param_groups[0]["lr"]

      total_loss += loss.item()

      global_step += 1

      if global_step % freq == 0:
        last_loss = total_loss / freq

        print(f"  Batch: {global_step} | Loss: {last_loss:.2f}")
        total_loss = 0.

  return last_loss, lr

def evaluate(model, val_loader, i, device):
  val_steps = 0
  total_val_loss = 0.

  preds, labels = [], []

  model.eval()

  with torch.no_grad():
    for x_batch, y_batch in val_loader:

      x_batch , y_batch = x_batch.to(device), y_batch.to(device)

      val_outputs = model(x_batch)

      if val_outputs.size(0) != y_batch.size(0):
        print(f"Shape mismatch: val_outputs size {val_outputs.size(0)}, y_batch size {y_batch.size(0)}")

      loss = loss_fn(val_outputs, y_batch.long().to(device))

      preds.extend(torch.argmax(val_outputs, dim=1).cpu().numpy())
      labels.extend(y_batch.cpu().numpy())

      total_val_loss += loss.item()
      val_steps += 1

  accuracy = accuracy_score(labels, preds)
  f1 = f1_score(labels, preds, average='weighted')

  return total_val_loss, val_steps, preds, labels, f1, accuracy

def train(model, data_folder, optimizer,
          scheduler, loss_fn, epochs,
          batch_size, n_splits, window_size,
          stride, freq, class_names):

  device = "cuda" if torch.cuda.is_available() else "cpu"

  model.to(device)

  X, y = [], []

  print("Processing data...")
  dataset = LMGDataset(data_folder, window_size, stride)
  for i in range(len(dataset)):
    data, label = dataset[i]
    X.append(data.numpy())
    y.append(label.numpy())

  X = np.array(X)
  y = np.array(y)

  dataset_length = len(X)

  train_losses, val_losses = [], []
  all_preds, all_labels = [], []

  tscv = TimeSeriesSplit(n_splits)

  for i, (train_index, test_index) in enumerate(tscv.split(X)):

    if test_index[-1] >= dataset_length:
        print(f"Skipping split {i + 1}: Test index exceeds dataset bounds. {test_index[-1]}")
        continue

    X_train, X_test = np.array(X)[train_index, :], np.array(X)[test_index, :]
    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]

    train_dataset = torch.utils.data.TensorDataset(
          torch.tensor(X_train, dtype=torch.float32),
          torch.tensor(y_train, dtype=torch.float32)
        )
    val_dataset = torch.utils.data.TensorDataset(
          torch.tensor(X_test, dtype=torch.float32),
          torch.tensor(y_test, dtype=torch.float32)
        )

    train_loader = DataLoader(train_dataset, batch_size, shuffle=None)
    val_loader = DataLoader(val_dataset, batch_size, shuffle=None)

    print(f"\nSplit: {i + 1} | Total Batch: {len(train_loader)}\n")

    for epoch in range(epochs):
      print(f"EPOCH: {epoch + 1}")
      print("------------------------------------------------")

      model.train()

      avg_loss, lr = train_batch(model, train_loader, optimizer, loss_fn, freq, device)
      scheduler.step()

      total_val_loss, val_steps, preds, labels, f1, accuracy = evaluate(model, val_loader, i, device)

      all_preds.extend(preds)
      all_labels.extend(labels)

      avg_val_loss = total_val_loss / val_steps
      print(f"LOSS --> Train Loss: {avg_loss:.2f} | Valid Loss: {avg_val_loss:.2f} | Lr: {lr:.4f} | F1: {f1:.2f} | Accuracy: {accuracy:.2f}\n")

      train_losses.append(avg_loss)
      val_losses.append(avg_val_loss)

  overall_accuracy = accuracy_score(all_labels, all_preds)
  overall_f1 = f1_score(all_labels, all_preds, average="weighted")

  print(f"Overall Accuracy: {overall_accuracy:.2f}")
  print(f"Overall F1 Score: {overall_f1:.2f}\n")

  plot_confusion_matrix(all_labels, all_preds, class_names)

  return train_losses, val_losses

def plot_losses(epochs, train_losses, val_losses):

  plt.figure()

  fig, ax = plt.subplots()

  ax.plot(epochs, train_losses, label="Train Loss")
  ax.plot(epochs, val_losses, linestyle="-.", label="Validation loss")

  ax.set_xlabel("Epochs")
  ax.set_ylabel("Loss")
  ax.legend(loc="upper right")

  fig.tight_layout()

train_losses, val_losses = train(
      model,
      data_folder,
      optimizer,
      scheduler,
      loss_fn,
      epochs,
      batch_size,
      n_splits,
      window_size,
      stride,
      freq,
      class_names
    )

plot_losses(range(1, (epochs * n_splits)+1), train_losses, val_losses)